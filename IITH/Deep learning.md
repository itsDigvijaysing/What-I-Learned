# DL - 00
- Online Lecture
- Evaluation Grading Criteria:
  1. Programming Assignment- 30% (best 3 of 4)
  2. Written exams (best 3 of 4; pre-scheduled) - 30%
  3. Kaggle Project (presentation) - 20%
  4. Endsem (written) - 20%

![Deep Learning TA](Deep%20learning%20TAs.png)

- **Course for Deep Learning Content:** 
	- Starting from an artificial neuron model, the aim of this course is to understand feed-forward, recurrent architectures of Artificial Neural Networks, all the way to the latest Generative Al models driven by Deep Neural Networks. Specifically, we will discuss the basic Neuron models (McCulloch Pitts, Perceptron), Multi-Layer Perceptron (MLP), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN, LSTM and GRU). We will understand these models' representational ability and how to train them using the Gradient Descent technique using the Backpropagation algorithm. We will then discuss the encoder-decoder architecture, attention mechanism and its variants. That will be followed by self-attention and Transformers. The next part of the course will be on Generative Al, wherein we will discuss Variational Autoencoders, GANs, Diffusion Models, GPT, BERT, etc. We will briefly discuss multi-modal representation learning (e.g., CLIP). Towards the end, students will be briefly exposed to some of the advanced topics and/or recent trends in deep learning.

- LLM Model compute used during training:
![Compute power Used](Deep%20learning%20model%20compute.jpg)
	|Main Tech|Language|Licence|Main Backer|
	|—|—|—|—|
	|PyTorch|Python, C++|BSD|Facebook|
	|TensorFlow|Python, C++|Apache|Google|
	|JAX|Python|Apache|Google|
	|MXNet|Python, C++, R, Scala|Apache|Amazon|
	|CNTK|Python, C++|MIT|Microsoft|
	|Torch|Lua|BSD|Facebook|
	|Theano|Python|BSD|U. of Montreal|
	|Caffe|C++|BSD 2 clauses|U. of CA, Berkeley|

- **Reference Material:**
	- Sir Channel Link: [Deep Learning (AI2100, AI5100 and CS5480) Course Contents - Deep Learning / Jan-May 2025](https://krmopuri.github.io/dl25/)
	- Deep Learning Foundations and Concepts by C Bishop & H Bishop
	- ﻿﻿Michael Nielsen's text book on NN & DL
	- ﻿﻿NPTEL course on Deep Learning by Prof. Mitesh Khapra, IITM
	- ﻿﻿DL course by François Fleuret, Uni. of Geneva
	- ﻿﻿Deep Learning textbook by lan Goodfellow et al.
	- ﻿﻿PyTorch - https://pytorch.org/
# DL - 01
- threshold logic unit 
- check the story of McCulloch & Pitts.
- Boolean inputs & output. { inputs can be excitatory or inhibitory nature }
- when inhibitory input →1 output →o 
- ==check== logical operation & its arithmetic things.
- perception started , weights & biased relation.
- usage of bias as it can be important point to reach final output.

![DL 01](Deep%20learning%20-%2001.png)

# DL - 02

**absent… (attended through mirror)**
- Line Classifier- Shortcomings
- what kind of preprocessing do we have to do, Example with XOR function
- They found something like:
	- Any boolean function of n inputs can be exactly represented wit 2^n perceptions in the hidden layer and 1 in the output layer!

# DL - 03
- Lets see what multiple perceptron can do.
- Going to do in notepad for checking.
- ==End of Month We have Test==
- Anything more than 2 hidden layer is deep neural network.
- Smallest no. Of neuron require to learn XOR function.
![DL 03](Deep%20learning%2003.png)
# DL - 04
- Gradient descent
- All variants (stochastic gradient descent, batch descent…)


# DL - 05

- Chain rule of differential calculus
- Distributed chain rule of differential calculus.
- Some other concepts as well.

![](Deep%20learning%20DL%20-%205.png)

# DL - 06

- Differentiation function of Back-propagation.
- Notes by friend kshitij.
![Deep learning till 22 Jun](Deep%20learning%20till%2022%20Jun.pdf)

# DL - 07

- Information Theory
- Cross Entropy Loss
- KL-Divergence

![](DL%20cross%20entropy%20Info%20Theroy.png)

# DL - 08

- Convergence of Quadratic Function
- Generic Convex Function
- Multivariate Function
- There is bound like make sure bound of Optimal step should be greater than 1/C and less than 2/C to have effective Learning Rate
- Check Derivatives & total parameters & Numerical concepts

![](DL%20gradient%20quadratic%20function.png)

# DL - 09

- Deep Regularization
  1. Parameter Norm Penalty
  2. Dataset Augmentation
  3. Multi task Learning
  4. Dropout
  5. Batch Normalization
- What is gamma and Beta check

![](Deep%20learning%2009.png)


# DL - 10

![](Deep%20learning%2010.png)

# DL - 11

![](DL%2011.png)

# DL - 12

![](Deep%20learning%2012.png)

# DL - 13

![](Deep%20learning%2013.png)

# DL - 14

![](Deep%20learning%2014.png)

