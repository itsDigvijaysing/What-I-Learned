# DL - 00
- Online Lecture
- Evaluation Grading Criteria:
  1. Programming Assignment- 30% (best 3 of 4)
  2. Written exams (best 3 of 4; pre-scheduled) - 30%
  3. Kaggle Project (presentation) - 20%
  4. Endsem (written) - 20%

![Deep Learning TA](Deep%20learning%20TAs.png)

- **Course for Deep Learning Content:** 
	- Starting from an artificial neuron model, the aim of this course is to understand feed-forward, recurrent architectures of Artificial Neural Networks, all the way to the latest Generative Al models driven by Deep Neural Networks. Specifically, we will discuss the basic Neuron models (McCulloch Pitts, Perceptron), Multi-Layer Perceptron (MLP), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN, LSTM and GRU). We will understand these models' representational ability and how to train them using the Gradient Descent technique using the Backpropagation algorithm. We will then discuss the encoder-decoder architecture, attention mechanism and its variants. That will be followed by self-attention and Transformers. The next part of the course will be on Generative Al, wherein we will discuss Variational Autoencoders, GANs, Diffusion Models, GPT, BERT, etc. We will briefly discuss multi-modal representation learning (e.g., CLIP). Towards the end, students will be briefly exposed to some of the advanced topics and/or recent trends in deep learning.

- **Reference Material:**
	- Sir Channel Link: [Deep Learning (AI2100, AI5100 and CS5480) Course Contents - Deep Learning / Jan-May 2025](https://krmopuri.github.io/dl25/)
	- Deep Learning Foundations and Concepts by C Bishop & H Bishop
	- ﻿﻿Michael Nielsen's text book on NN & DL
	- ﻿﻿NPTEL course on Deep Learning by Prof. Mitesh Khapra, IITM
	- ﻿﻿DL course by François Fleuret, Uni. of Geneva
	- ﻿﻿Deep Learning textbook by lan Goodfellow et al.
	- ﻿﻿PyTorch - https://pytorch.org/
# DL - 01
- threshold logic unit 
- check the story of McCulloch & Pitts.
- Boolean inputs & output. { inputs can be excitatory or inhibitory nature }
- when inhibitory input →1 output →o 
- ==check== logical operation & its arithmetic things.
- perception started , weights & biased relation.
- usage of bias as it can be important point to reach final output.

![](Deep%20learning%20-%2001.png)

# DL - 02

absent… (attended through mirror)

# DL - 03

Lets see what multiple perceptron can do.
Going to do in notepad for checking.
- ==End of Month We have Test==
- Anything more than 2 hidden layer is deep neural network.
![](Deep%20learning%2003.png)



